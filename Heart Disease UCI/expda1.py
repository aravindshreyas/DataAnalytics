# -*- coding: utf-8 -*-
"""ExpDA1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15daN4SSpIHAqueLl-EiAcs_6Ho6a6Gzz

Model to predict heart disease by Aravind Shreyas Ramesh.

UCI Heart diesease dataset (Explaratory data analysis)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("heart.csv")

plt.hist(df['age'])

plt.hist(df['target'])

"""It's great that we more or less have the same number of both labels. If the number of each label is equally distributed, we can expect better results during regression and/or classification.

As we can see, there are 303 entries. The target value 0 suggests the patient doesn't have heart disease, while 1 indicates the patient indeed does have heart disease.

Since we have just two labels, I personally beleive logistic regression is the way to go. Other methods of classification could be SVM or even RVM for better results relative to SVM.
"""

df.head()

sns.pairplot(data=df,hue='target')

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(df.drop('target',axis=1))
new_feat = scaler.transform(df.drop('target',axis=1))
df_feat = pd.DataFrame(new_feat,columns=df.columns[:-1])

"""Pairplots are so powerful for checking trends. It makes it much easier to figure out which attributes to look out for to identify trends."""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_feat, df['target'], test_size=0.25)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

lr.fit(X_train,y_train)

pred = lr.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y_test,pred))

print(classification_report(y_test,pred))

"""[[38  3]
 [ 4 31]]
        
        
              precision    recall  f1-score   support

           0       0.90      0.93      0.92        41
           1       0.91      0.89      0.90        35

    accuracy                           0.91        76
   macro avg       0.91      0.91      0.91        76
weighted avg       0.91      0.91      0.91        76

Since this application is of medical diagnosis, it's critical to have 0 or less number of false negatives. Even false positives is acceptable but false negatives are disastrous.

Thus, I have managed to get precision of upto 0.9 which is not the best for this application, but definitely not a bad result especially with the false negatives being near 0.


"""
